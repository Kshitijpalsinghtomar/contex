<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation — Contex</title>
    <meta name="description"
        content="Contex documentation: installation, quick start, SDK middleware, Budget Engine, TENS IR, API reference, and guides.">
    <link rel="stylesheet" href="./style.css">
    <script>
        const theme = localStorage.getItem('contex-theme') || 'dark';
        document.documentElement.setAttribute('data-theme', theme);
    </script>
</head>

<body>

    <nav class="nav">
        <a href="/" class="nav-brand">
            <div class="logo-icon">C</div>
            Contex
        </a>
        <ul class="nav-links">
            <li><a href="./">Home</a></li>
            <li><a href="./docs.html" class="active">Docs</a></li>
            <li><a href="./benchmarks.html">Benchmarks</a></li>
        </ul>
        <div class="nav-actions">
            <button class="btn btn-ghost btn-sm" id="theme-toggle" aria-label="Toggle theme">
                <!-- Sun Icon -->
                <svg class="sun-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
                    <circle cx="12" cy="12" r="5"></circle>
                    <line x1="12" y1="1" x2="12" y2="3"></line>
                    <line x1="12" y1="21" x2="12" y2="23"></line>
                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                    <line x1="1" y1="12" x2="3" y2="12"></line>
                    <line x1="21" y1="12" x2="23" y2="12"></line>
                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                </svg>
                <!-- Moon Icon -->
                <svg class="moon-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                    stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
            </button>
            <a href="https://github.com/contex/contex" class="btn btn-ghost btn-sm" target="_blank">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                    stroke-linecap="round" stroke-linejoin="round" style="margin-right: 6px;">
                    <path
                        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                    </path>
                </svg>
                GitHub
            </a>
        </div>
    </nav>

    <div class="docs-layout">
        <aside class="sidebar">
            <div class="sidebar-section">
                <h4>Getting Started</h4>
                <a href="#installation" class="active">Installation</a>
                <a href="#quick-start">Quick Start</a>
                <a href="#project-structure">Project Structure</a>
            </div>
            <div class="sidebar-section">
                <h4>Core Concepts</h4>
                <a href="#how-it-works">How It Works</a>
                <a href="#budget-engine">Budget Engine</a>
                <a href="#tens-ir">TENS Binary IR</a>
                <a href="#tens-text">TENS-Text Format</a>
                <a href="#format-selection">Format Auto-Selection</a>
                <a href="#token-counting">Token Counting</a>
            </div>
            <div class="sidebar-section">
                <h4>SDK Middleware</h4>
                <a href="#openai-middleware">OpenAI Integration</a>
                <a href="#anthropic-middleware">Anthropic Integration</a>
                <a href="#placeholder-syntax">Placeholder Syntax</a>
            </div>
            <div class="sidebar-section">
                <h4>API Reference</h4>
                <a href="#contex-class">Contex Class</a>
                <a href="#model-registry">Model Registry</a>
                <a href="#context-storage">Context Storage</a>
                <a href="#tens-encoder">TENS Encoder/Decoder</a>
            </div>
            <div class="sidebar-section">
                <h4>Advanced</h4>
                <a href="#prefix-caching">Prefix Caching</a>
                <a href="#custom-models">Custom Models</a>
                <a href="#benchmark-suite">Benchmark Suite</a>
                <a href="#methodology">Methodology</a>
            </div>
            <div class="sidebar-section">
                <h4>Moat Features</h4>
                <a href="#dedup-cache">Cross-Session Dedup</a>
                <a href="#predictive-packer">Predictive Packer</a>
                <a href="#pretokenized-blocks">Pre-Tokenized Blocks</a>
            </div>
        </aside>

        <main class="docs-content">
            <h1>Contex Documentation</h1>
            <p>A structured execution engine for LLMs. 59% fewer tokens. 6.8x faster decoding. Zero information loss.
            </p>

            <!-- Installation -->
            <h2 id="installation">Installation</h2>

            <h3>Package Manager</h3>
            <pre>pnpm add @contex/core @contex/engine</pre>
            <pre>npm install @contex/core @contex/engine</pre>
            <pre>yarn add @contex/core @contex/engine</pre>

            <h3>With SDK Middleware (recommended)</h3>
            <pre>pnpm add @contex/core @contex/engine @contex/middleware</pre>

            <div class="callout info">
                <strong>Requirements:</strong> Node.js 18+ · TypeScript 5+ (optional but recommended)
            </div>

            <!-- Quick Start -->
            <h2 id="quick-start">Quick Start</h2>

            <h3>1. Analyze with CLI</h3>
            <pre># Inspect savings potential
npx contex savings data.json

# Output:
#   Original:  22,000 tokens
#   Contex:    14,205 tokens (-35%)</pre>

            <h3>2. Materialize Tokens</h3>
            <pre># Pre-compute tokens for specific model
npx contex materialize data.json --model gpt-4o</pre>

            <h3>3. Inject via Middleware</h3>
            <pre>import { createContexOpenAI } from '@contex/middleware';

// Wrap your client
const client = createContexOpenAI(new OpenAI(), {
  data: { 
    users: myUserData // Injects as {{CONTEX:users}}
  }
});

// Use in prompt
await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'Analyze {{CONTEX:users}}' }]
});</pre>

            <!-- Project Structure -->
            <h2 id="project-structure">Project Structure</h2>
            <pre>@contex/
├── core/           # Format engine: TENS, tokenizer, formatters
├── engine/         # Context brain: budget, caching, storage
├── middleware/     # OpenAI + Anthropic SDK interceptors
├── cli/            # Benchmark suite + CLI tools
├── server/         # REST API (Hono + Zod)
└── tens-wasm/      # Rust-based TENS encoder (experimental)</pre>

            <div class="callout tip">
                <strong>Tip:</strong> Most users only need <code>@contex/engine</code> +
                <code>@contex/middleware</code>. The engine re-exports everything from core.
            </div>

            <!-- How It Works -->
            <h2 id="how-it-works">How It Works</h2>
            <p>Contex solves the "60% structural waste" problem in LLM context windows. JSON repeats keys, braces, and
                quotes for every row. At 1,000 rows, this wastes ~35,000 tokens — tokens you're paying for.</p>

            <h3>The Pipeline</h3>
            <ol>
                <li><strong>Ingest:</strong> Store structured data via <code>ctx.insert(collection, rows)</code></li>
                <li><strong>Budget:</strong> Calculate available tokens after system prompt, reserves, and response
                    budget</li>
                <li><strong>Format Trial:</strong> Serialize data in all candidate formats and count tokens with the
                    model's actual tokenizer</li>
                <li><strong>Select:</strong> Pick the format with the fewest tokens that fits the budget</li>
                <li><strong>Row Fitting:</strong> Binary search for maximum rows that fit</li>
                <li><strong>Emit:</strong> Return the packed string ready for prompt injection</li>
            </ol>

            <p>The key insight: <strong>token counts vary per tokenizer</strong>. CSV might beat Markdown on GPT-4o but
                lose on Claude. Contex tests every format against the actual model's tokenizer to find the true optimum.
            </p>

            <!-- Budget Engine -->
            <h2 id="budget-engine">Budget Engine</h2>
            <p>The budget engine calculates the maximum data that fits into a model's context window.</p>

            <pre>const budget = ctx.calculateBudget('gpt-4o', {
  maxContextTokens: 128000,  // Model's max context
  systemPromptTokens: 800,   // Your system prompt
  reserveTokens: 2000,       // Response buffer
  existingContentTokens: 0,  // Already-used context
});

// budget.availableTokens → 125,200
// budget.maxRows → 5,240 (in TENS format)</pre>

            <h3>Model Awareness</h3>
            <p>The budget engine knows each model's pricing, context window, and tokenizer. It uses this to calculate
                cost projections and optimal format selection.</p>

            <pre>ctx.getModelInfo('gpt-4o');
// {
//   maxContext: 128000,
//   inputPrice: 2.50,     // per 1M tokens
//   outputPrice: 10.00,
//   encoding: 'o200k_base'
// }</pre>

            <!-- TENS IR -->
            <h2 id="tens-ir">TENS Binary IR</h2>
            <p><strong>TENS</strong> (Token Encoded Native Structure) is the canonical internal representation. It's not
                a transport format — it's an LLM-aware structural IR.</p>

            <h3>Why Not MessagePack / Protobuf / CBOR?</h3>
            <p>Those are <em>transport</em> formats designed for machine-to-machine communication. They optimize for
                byte size, not token count. LLMs don't parse bytes — they read tokens. A format that's 50% smaller in
                bytes might use the <em>same</em> number of tokens.</p>

            <h3>TENS Design Principles</h3>
            <ul>
                <li><strong>Schema deduplication:</strong> Column names defined once, not per row</li>
                <li><strong>Positional values:</strong> Fields identified by position, not by string key</li>
                <li><strong>Sorted keys:</strong> Deterministic ordering → identical output → KV cache hits</li>
                <li><strong>Binary IR:</strong> 233K encode, 870K decode ops/sec. Direct memory mapping.</li>
            </ul>

            <h3>Performance</h3>
            <table class="benchmark-table" style="max-width: 500px;">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                        <th>vs JSON</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Encoding</td>
                        <td class="mono">233,000 ops/s</td>
                        <td class="saving">4.2x faster</td>
                    </tr>
                    <tr>
                        <td>Decoding</td>
                        <td class="mono">870,610 ops/s</td>
                        <td class="saving">6.8x faster</td>
                    </tr>
                    <tr>
                        <td>Payload</td>
                        <td class="mono">59,637 B</td>
                        <td class="saving">-76.8%</td>
                    </tr>
                    <tr>
                        <td>Memory</td>
                        <td class="mono">12 MB</td>
                        <td class="saving">-74%</td>
                    </tr>
                </tbody>
            </table>

            <!-- TENS-Text -->
            <h2 id="tens-text">TENS-Text Format</h2>
            <p><strong>TENS-Text</strong> (<code>.tens</code> files) is a lossless, human-readable text format that maps
                1:1 to TENS binary. Use it for debugging, inspecting data, or any workflow where human readability
                matters.</p>

            <h3>Key Properties</h3>
            <ul>
                <li><strong>Indentation-based:</strong> 2-space indent for field lines, no brackets or commas</li>
                <li><strong>Schema dedup:</strong> <code>@schema</code> directive defines fields once — not repeated per
                    row</li>
                <li><strong>Dictionary compression:</strong> <code>@dict</code> stores repeated strings, referenced as
                    <code>@0</code>, <code>@1</code>, etc.
                </li>
                <li><strong>Array via repetition:</strong> Same field name repeated = array, no <code>[]</code> syntax
                    in data</li>
                <li><strong>Type-directed parsing:</strong> Schema types prevent accidental coercion
                    (<code>str</code>-typed <code>42</code> stays string)</li>
                <li><strong>Deterministic:</strong> Same input → identical bytes → prefix cache friendly</li>
            </ul>

            <h3>Example</h3>
            <pre>@version 1
@encoding o200k_base
@schema ticket id:num title:str status:str tag:str[]

@dict open closed

ticket
  id 1
  title "Bug in auth"
  status @0
  tag security
  tag backend
ticket
  id 2
  title "UI polish"
  status @1</pre>

            <h3>Programmatic Usage</h3>
            <pre>import { TensTextEncoder, TensTextDecoder } from '@contex/core';

// Encode → human-readable text
const encoder = new TensTextEncoder('o200k_base');
const text = encoder.encode(data, 'ticket');

// Decode → lossless roundtrip
const decoder = new TensTextDecoder();
const { data: restored } = decoder.decode(text);</pre>

            <div class="callout info">
                <strong>Specification:</strong> See the full formal grammar (EBNF), type system, error handling
                behavior, and design rationale in the <a
                    href="https://github.com/contex/contex/blob/main/docs/tens-specification.md#63-tens-text-language-specification-tens"
                    style="color: var(--accent-blue);">TENS-Text Language Specification §6.3</a>.
            </div>

            <!-- Format Selection -->
            <h2 id="format-selection">Format Auto-Selection</h2>
            <p>Contex doesn't commit to a single format. It evaluates 10 formats against the target model's tokenizer
                and picks the winner.</p>

            <h3>Supported Formats</h3>
            <ul>
                <li><strong>TENS</strong> — Positional token-stream (usually wins, -59%)</li>
                <li><strong>CSV</strong> — Good for homogeneous tabular data (-31%)</li>
                <li><strong>TOON</strong> — Tab-delimited object notation (-32%)</li>
                <li><strong>Markdown</strong> — Table format (-24%)</li>
                <li><strong>JSON</strong> — Fallback when structure is irregular</li>
                <li><strong>NDJSON / JSON-Pretty / XML / YAML</strong> — Never selected (worse than JSON)</li>
            </ul>

            <!-- Token Counting -->
            <h2 id="token-counting">Token Counting</h2>
            <p>Contex uses <code>tiktoken</code> for exact token counting — the same library OpenAI uses.</p>

            <pre>import { TokenizerManager } from '@contex/core';

const tokenizer = new TokenizerManager('o200k_base');
const count = tokenizer.countTokens('Hello, world!');
// count → 4</pre>

            <div class="callout warn">
                <strong>Important:</strong> Token counts differ between encodings. <code>o200k_base</code> (GPT-4o) and
                <code>cl100k_base</code> (Claude/Gemini) produce different counts for the same text. Contex handles this
                automatically based on the target model.
            </div>

            <!-- OpenAI Middleware -->
            <h2 id="openai-middleware">OpenAI Middleware</h2>
            <p>Drop-in interceptor that replaces <code>{{CONTEX:collection}}</code> placeholders with budget-optimized
                data.</p>

            <pre>import OpenAI from 'openai';
import { Contex } from '@contex/engine';
import { createOpenAIMiddleware } from '@contex/middleware';

const ctx = new Contex();
const openai = new OpenAI();

// Store data
await ctx.insert('tickets', ticketData);

// Create middleware
const middleware = createOpenAIMiddleware(ctx, openai);

// Use placeholders — middleware replaces them automatically
const response = await middleware.chat({
  model: 'gpt-4o',
  messages: [
    { role: 'system', content: 'You are a support analyst.' },
    {
      role: 'user',
      content: 'Here are the open tickets:\n{{CONTEX:tickets}}\n\nSummarize the critical ones.',
    },
  ],
});

// Under the hood:
// 1. Detected model is 'gpt-4o' (128K context, o200k_base encoding)
// 2. Calculated token budget after system prompt
// 3. Tried 10 formats, TENS won (-59% vs JSON)
// 4. Binary-searched for max rows that fit
// 5. Replaced {{CONTEX:tickets}} with packed TENS output
// 6. Forwarded to OpenAI API normally</pre>

            <!-- Anthropic Middleware -->
            <h2 id="anthropic-middleware">Anthropic Middleware</h2>
            <pre>import Anthropic from '@anthropic-ai/sdk';
import { Contex } from '@contex/engine';
import { createAnthropicMiddleware } from '@contex/middleware';

const ctx = new Contex();
const anthropic = new Anthropic();

await ctx.insert('orders', orderData);

const middleware = createAnthropicMiddleware(ctx, anthropic);

const response = await middleware.messages.create({
  model: 'claude-4-sonnet',
  max_tokens: 4096,
  messages: [
    {
      role: 'user',
      content: 'Analyze these orders:\n{{CONTEX:orders}}\n\nFind anomalies.',
    },
  ],
});
</pre>

            <!-- Gemini Middleware -->
            <h2 id="gemini-middleware">Gemini Middleware</h2>
            <pre>import { GoogleGenerativeAI } from '@google/generative-ai';
import { Contex } from '@contex/engine';
import { createContexGemini } from '@contex/middleware';

const ctx = new Contex();
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY!);

// Create middleware wrapper around specific model
const model = createContexGemini(
  genAI.getGenerativeModel({ model: 'gemini-2.0-flash' }),
  ctx,
  'gemini-2.0-flash'
);

// Use placeholders in prompt
const result = await model.generateContent(
  'Analyze these tickets: {{CONTEX:tickets}}'
);</pre>
            </pre>

            <!-- Placeholder Syntax -->
            <h2 id="placeholder-syntax">Placeholder Syntax</h2>
            <pre>{{CONTEX:collection_name}}              // Basic — use all available budget
{{CONTEX:collection_name:maxRows=100}}   // Limit to 100 rows
{{CONTEX:collection_name:format=csv}}    // Force CSV format
{{CONTEX:collection_name:budget=4000}}   // Custom token budget</pre>

            <!-- API Reference -->
            <h2 id="contex-class">Contex Class</h2>
            <pre>import { Contex } from '@contex/engine';

const ctx = new Contex(encoding?: string);

// Data operations
await ctx.insert(collection: string, rows: object[]);
await ctx.get(collection: string): object[];
await ctx.clear(collection: string);

// Packing
await ctx.packForModel(model: string, collection: string, options?: PackOptions);

// Model info
ctx.getModelInfo(model: string): ModelInfo;
ctx.listModels(): string[];</pre>

            <h3>PackOptions</h3>
            <pre>interface PackOptions {
  maxTokens?: number;      // Token budget for this data
  reserveTokens?: number;  // Reserve for system prompt / response
  format?: string;         // Force a specific format (skip auto-selection)
  maxRows?: number;        // Maximum rows to include
}</pre>

            <!-- Model Registry -->
            <h2 id="model-registry">Model Registry</h2>
            <p>33 production models with verified pricing (Feb 2026), context windows, and tokenizer mappings. See <a
                    href="https://pricepertoken.com" style="color: var(--accent-blue);">pricepertoken.com</a> for live
                rates.</p>

            <div style="overflow-x: auto;">
                <table class="benchmark-table" style="font-size: 13px;">
                    <thead>
                        <tr>
                            <th>Model</th>
                            <th>Context</th>
                            <th>Input $/1M</th>
                            <th>Encoding</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td>gpt-5</td>
                            <td class="mono">1M</td>
                            <td class="mono">$1.25</td>
                            <td class="mono">o200k</td>
                        </tr>
                        <tr>
                            <td>gpt-4o</td>
                            <td class="mono">128K</td>
                            <td class="mono">$2.50</td>
                            <td class="mono">o200k</td>
                        </tr>
                        <tr>
                            <td>gpt-4.1</td>
                            <td class="mono">1M</td>
                            <td class="mono">$2.00</td>
                            <td class="mono">o200k</td>
                        </tr>
                        <tr>
                            <td>gpt-4.1-mini</td>
                            <td class="mono">1M</td>
                            <td class="mono">$0.40</td>
                            <td class="mono">o200k</td>
                        </tr>
                        <tr>
                            <td>gpt-4.1-nano</td>
                            <td class="mono">1M</td>
                            <td class="mono">$0.10</td>
                            <td class="mono">o200k</td>
                        </tr>
                        <tr>
                            <td>claude-4.5-sonnet</td>
                            <td class="mono">1M</td>
                            <td class="mono">$3.00</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>claude-4.6-opus</td>
                            <td class="mono">1M</td>
                            <td class="mono">$5.00</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>claude-3.7-sonnet</td>
                            <td class="mono">200K</td>
                            <td class="mono">$3.00</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>gemini-2.5-pro</td>
                            <td class="mono">1M</td>
                            <td class="mono">$1.25</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>gemini-2.5-flash</td>
                            <td class="mono">1M</td>
                            <td class="mono">$0.30</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>llama-4-maverick</td>
                            <td class="mono">1M</td>
                            <td class="mono">$0.15</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>llama-4-scout</td>
                            <td class="mono">1M</td>
                            <td class="mono">$0.08</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>grok-3</td>
                            <td class="mono">131K</td>
                            <td class="mono">$3.00</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>deepseek-v3.2</td>
                            <td class="mono">128K</td>
                            <td class="mono">$0.28</td>
                            <td class="mono">cl100k</td>
                        </tr>
                        <tr>
                            <td>mistral-large-2</td>
                            <td class="mono">128K</td>
                            <td class="mono">$2.00</td>
                            <td class="mono">cl100k</td>
                        </tr>
                    </tbody>
                </table>
            </div>

            <div class="callout tip">
                <strong>Tip:</strong> Override pricing at runtime:
                <code>ctx.packForModel('gpt-4o', 'data', { modelOverrides: { inputPrice: 2.00 } })</code>
            </div>

            <!-- Context Storage -->
            <h2 id="context-storage">Context Storage</h2>
            <p>In-memory key-value storage for collections. Designed for ephemeral context injection, not persistence.
            </p>

            <pre>await ctx.insert('users', data);          // Store
const users = await ctx.get('users');       // Retrieve
await ctx.clear('users');                   // Delete
const collections = ctx.listCollections();  // List all</pre>

            <!-- TENS Encoder -->
            <h2 id="tens-encoder">TENS Encoder / Decoder</h2>
            <pre>import { TENSEncoder, TENSDecoder } from '@contex/core';

// Encode
const encoder = new TENSEncoder();
const binary = encoder.encode([
  { id: 1, name: 'Alice', role: 'admin' },
  { id: 2, name: 'Bob',   role: 'user' },
]);
// binary: Uint8Array (compact binary representation)

// Decode
const decoder = new TENSDecoder();
const rows = decoder.decode(binary);
// rows: [{ id: 1, name: 'Alice', role: 'admin' }, ...]</pre>

            <!-- Prefix Caching -->
            <h2 id="prefix-caching">Prefix Caching</h2>
            <p>TENS produces <strong>deterministic output</strong> — same input always produces the same string. This
                enables prefix caching on inference servers.</p>

            <h3>How it works</h3>
            <ol>
                <li>TENS sorts keys alphabetically before serialization</li>
                <li>Values are emitted in canonical positional order</li>
                <li>Two calls with overlapping data produce identical prefixes</li>
                <li>The inference server's KV cache recognizes the prefix → skips re-computation</li>
            </ol>

            <h3>Compatible Servers</h3>
            <ul>
                <li><strong>vLLM:</strong> Automatic prefix caching (enable with <code>--enable-prefix-caching</code>)
                </li>
                <li><strong>SGLang:</strong> RadixAttention — built-in prefix sharing</li>
                <li><strong>TensorRT-LLM:</strong> KV cache reuse across requests</li>
                <li><strong>OpenAI API:</strong> Automatic prefix caching for prompts >1024 tokens</li>
            </ul>

            <div class="callout info">
                <strong>Impact:</strong> 40-60% latency reduction on repeated queries with overlapping context data.
            </div>

            <!-- Custom Models -->
            <h2 id="custom-models">Custom Models</h2>
            <p>Register your own models or fine-tunes:</p>
            <pre>ctx.registerModel('my-fine-tune', {
  maxContext: 32000,
  inputPrice: 1.50,       // per 1M tokens
  outputPrice: 4.00,
  encoding: 'cl100k_base',
});</pre>

            <!-- Benchmark Suite -->
            <h2 id="benchmark-suite">Running Benchmarks</h2>
            <pre># Run the full benchmark suite
npx tsx packages/cli/src/benchmark.ts

# This generates:
# - packages/cli/benchmark_report.html (interactive report)
# - Terminal output with summary tables</pre>

            <p>The benchmark suite tests 24 datasets across 10 formats at 6 scale tiers. All results are deterministic —
                seeded PRNG ensures identical output across runs.</p>

            <a href="./benchmarks.html" style="color: var(--accent-blue); text-decoration: none; font-weight: 500;">→
                View Full Benchmark Results</a>

            <!-- Methodology -->
            <h2 id="methodology">Methodology</h2>
            <p>All benchmarks follow research methodology:</p>
            <ul>
                <li><strong>Hardware:</strong> Apple M2 Pro / 16 GB RAM / NVMe SSD</li>
                <li><strong>Runtime:</strong> Node.js v20 LTS (V8, JIT)</li>
                <li><strong>Warm-up:</strong> 100 iterations discarded before measurement</li>
                <li><strong>Sample size:</strong> 1,000 iterations, median reported</li>
                <li><strong>Statistics:</strong> Median ± IQR, outliers >2σ excluded</li>
                <li><strong>GC:</strong> <code>--expose-gc</code>, forced between runs</li>
                <li><strong>Datasets:</strong> 24 seeded (PRNG seed: 0xC0FFEE), deterministic</li>
                <li><strong>Tokenizer:</strong> tiktoken (o200k_base + cl100k_base)</li>
            </ul>

            <div class="callout tip">
                <strong>Reproducibility:</strong> Run <code>npx tsx packages/cli/src/benchmark.ts</code> on any machine.
                Same seed → same results. No cherry-picking.
            </div>

            <!-- ═══ Moat: Cross-Session Dedup ═══ -->
            <h2 id="dedup-cache">Cross-Session Structural Dedup</h2>
            <p>The <code>StructuralDedupCache</code> tracks schemas and dictionary values across multiple encode
                operations. On subsequent encodes, only new schemas and values are emitted — cutting structural overhead
                to zero.</p>

            <h3>Basic Usage</h3>
            <pre>import { StructuralDedupCache } from '@contex/engine';

const cache = new StructuralDedupCache();

// First encode: full schema + dictionary emitted
const result1 = cache.encode([
  { id: 1, name: 'Alice', role: 'admin' },
  { id: 2, name: 'Bob', role: 'admin' }
]);
console.log(result1.isIncremental); // false

// Second encode: schema deduped, only new values
const result2 = cache.encode([
  { id: 3, name: 'Charlie', role: 'admin' }
]);
console.log(result2.isIncremental); // true
console.log(result2.newDictionaryValues); // ['Charlie']</pre>

            <h3>State Persistence</h3>
            <pre>// Serialize cache state for cross-session use
const state = cache.serialize();
localStorage.setItem('dedup-state', JSON.stringify(state));

// Restore in a new session
const restored = StructuralDedupCache.deserialize(state);</pre>

            <div class="callout info">
                <strong>Key benefit:</strong> For repetitive data (e.g., chat logs with the same schema), subsequent
                encodes produce near-zero structural overhead. Only new field <em>values</em> are transmitted.
            </div>

            <!-- ═══ Moat: Predictive Packer ═══ -->
            <h2 id="predictive-packer">Predictive Packer</h2>
            <p>The <code>packContext()</code> function solves the context packing problem: given a set of heterogeneous
                context items with different token costs and priorities, select the optimal subset that fits within a
                token budget.</p>

            <h3>Usage</h3>
            <pre>import { packContext } from '@contex/engine';

const items = [
  { id: 'sys', tokens: 200, priority: 10, content: '...' },
  { id: 'rag-1', tokens: 500, priority: 8, content: '...' },
  { id: 'rag-2', tokens: 300, priority: 6, content: '...' },
  { id: 'history', tokens: 800, priority: 4, content: '...' },
];

const result = packContext(items, {
  budget: 1000,
  strategy: 'knapsack' // or 'greedy' or 'density'
});

console.log(result.selected);   // ['sys', 'rag-1', 'rag-2']
console.log(result.totalTokens); // 1000
console.log(result.utilization); // 1.0</pre>

            <h3>Packing Strategies</h3>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Strategy</th>
                        <th>Algorithm</th>
                        <th>Best For</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>greedy</code></td>
                        <td>Sort by priority, take top-N</td>
                        <td>Simple, fast selection</td>
                    </tr>
                    <tr>
                        <td><code>density</code></td>
                        <td>Sort by priority/token ratio</td>
                        <td>Maximizing value density</td>
                    </tr>
                    <tr>
                        <td><code>knapsack</code></td>
                        <td>Dynamic programming (0/1)</td>
                        <td>Optimal budget utilization</td>
                    </tr>
                </tbody>
            </table>

            <!-- ═══ Moat: Pre-Tokenized Blocks ═══ -->
            <h2 id="pretokenized-blocks">Pre-Tokenized Binary Blocks</h2>
            <p>PTOK is a binary format that stores structured data with embedded token IDs. Once created, fields can be
                accessed and served without re-tokenizing — achieving near-zero latency on repeated serves.</p>

            <h3>Creating a Block</h3>
            <pre>import {
  createPreTokenizedBlock,
  readPreTokenizedBlock
} from '@contex/core';

const data = { id: 1, name: 'Alice', role: 'admin' };
const block = createPreTokenizedBlock(data, 'o200k_base');
// block is a Uint8Array in PTOK format</pre>

            <h3>Reading Fields</h3>
            <pre>const result = readPreTokenizedBlock(block);

// Full data access
console.log(result.data);       // { id: 1, name: 'Alice', role: 'admin' }
console.log(result.encoding);   // 'o200k_base'

// Field-level random access (no full decode needed)
console.log(result.fields.name.tokenIds); // [12345, 67890]
console.log(result.fields.name.value);    // 'Alice'</pre>

            <div class="callout tip">
                <strong>Performance:</strong> Pre-tokenized blocks skip the tokenizer entirely on repeated access.
                For hot data served across multiple requests, this eliminates the tokenization bottleneck.
            </div>
        </main>
    </div>

    <footer class="footer">
        <p>MIT © <a href="https://github.com/contex">Contex</a> · Built for production LLM pipelines</p>
    </footer>

    <script>
        // Theme Toggle Logic
        const toggleBtn = document.getElementById('theme-toggle');
        const sunIcon = toggleBtn.querySelector('.sun-icon');
        const moonIcon = toggleBtn.querySelector('.moon-icon');

        function updateIcon() {
            const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
            sunIcon.style.display = isDark ? 'none' : 'block';
            moonIcon.style.display = isDark ? 'block' : 'none';
            toggleBtn.style.color = isDark ? 'var(--text-muted)' : 'var(--accent-amber)';
        }

        // Init Icon
        updateIcon();

        toggleBtn.addEventListener('click', () => {
            const current = document.documentElement.getAttribute('data-theme');
            const next = current === 'dark' ? 'light' : 'dark';
            document.documentElement.setAttribute('data-theme', next);
            localStorage.setItem('contex-theme', next);
            updateIcon();
        });

        // Highlight active sidebar link on scroll
        const sections = document.querySelectorAll('[id]');
        const navLinks = document.querySelectorAll('.sidebar-section a');
        const observer = new IntersectionObserver(entries => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    navLinks.forEach(link => link.classList.remove('active'));
                    const active = document.querySelector(`.sidebar-section a[href="#${entry.target.id}"]`);
                    if (active) active.classList.add('active');
                }
            });
        }, { rootMargin: '-80px 0px -70% 0px' });
        sections.forEach(s => observer.observe(s));
    </script>
</body>

</html>