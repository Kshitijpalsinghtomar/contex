<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Documentation — Contex</title>
    <meta name="description"
        content="Contex documentation: installation, formats, pipeline, SDK middleware, TENS IR, API reference, and guides.">
    <link rel="stylesheet" href="./style.css">
    <script>
        const theme = localStorage.getItem('contex-theme') || 'dark';
        document.documentElement.setAttribute('data-theme', theme);
    </script>
    <style>
        .format-pipeline {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 12px;
            padding: 2rem;
            margin: 1.5rem 0;
            font-family: 'SF Mono', 'Cascadia Code', 'Fira Code', monospace;
            font-size: 0.85rem;
            line-height: 1.6;
            overflow-x: auto;
            white-space: pre;
        }

        .format-card {
            background: var(--bg-secondary);
            border: 1px solid var(--border);
            border-radius: 10px;
            padding: 1.25rem 1.5rem;
            margin: 1rem 0;
        }

        .format-card h4 {
            margin: 0 0 0.5rem 0;
            color: var(--accent);
            font-size: 0.95rem;
        }

        .format-card p {
            margin: 0.25rem 0;
            font-size: 0.9rem;
        }

        .format-card code {
            font-size: 0.82rem;
        }

        .tag {
            display: inline-block;
            padding: 2px 8px;
            border-radius: 4px;
            font-size: 0.75rem;
            font-weight: 600;
            margin-left: 6px;
        }

        .tag-default {
            background: var(--accent);
            color: #000;
        }

        .tag-debug {
            background: #6366f1;
            color: #fff;
        }

        .tag-internal {
            background: #64748b;
            color: #fff;
        }

        .docs-content h3 {
            margin-top: 1.75rem;
            margin-bottom: 0.75rem;
            color: var(--text);
            font-size: 1.15rem;
        }

        .divider {
            border: none;
            border-top: 1px solid var(--border);
            margin: 2.5rem 0;
        }

        .grid-2 {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1rem;
        }

        @media (max-width: 700px) {
            .grid-2 {
                grid-template-columns: 1fr;
            }
        }

        .callout {
            border-radius: 8px;
            padding: 1rem 1.25rem;
            margin: 1rem 0;
            font-size: 0.9rem;
        }

        .callout.info {
            background: rgba(99, 102, 241, 0.1);
            border-left: 3px solid #6366f1;
        }

        .callout.warn {
            background: rgba(245, 158, 11, 0.1);
            border-left: 3px solid #f59e0b;
        }

        .callout.tip {
            background: rgba(16, 185, 129, 0.1);
            border-left: 3px solid #10b981;
        }
    </style>
</head>

<body>

    <nav class="nav">
        <a href="./" class="nav-brand">
            <div class="logo-icon">C</div>
            Contex
        </a>
        <ul class="nav-links">
            <li><a href="./">Home</a></li>
            <li><a href="./docs.html" class="active">Docs</a></li>
            <li><a href="./tens-text.html">TENS-Text</a></li>
            <li><a href="./benchmarks.html">Benchmarks</a></li>
            <li><a href="./playground.html">Playground</a></li>
            <li><a href="./vision.html">Vision</a></li>
            <li><a href="./dashboard.html">Dashboard</a></li>
        </ul>
        <div class="nav-actions">
            <button class="btn btn-ghost btn-sm" id="theme-toggle" aria-label="Toggle theme">
                <svg class="sun-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                    stroke-width="2" stroke-linecap="round" stroke-linejoin="round" style="display: none;">
                    <circle cx="12" cy="12" r="5"></circle>
                    <line x1="12" y1="1" x2="12" y2="3"></line>
                    <line x1="12" y1="21" x2="12" y2="23"></line>
                    <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                    <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                    <line x1="1" y1="12" x2="3" y2="12"></line>
                    <line x1="21" y1="12" x2="23" y2="12"></line>
                    <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                    <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                </svg>
                <svg class="moon-icon" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor"
                    stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
                    <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                </svg>
            </button>
            <a href="https://github.com/kshitijpalsinghtomar/contex-llm" class="btn btn-ghost btn-sm" target="_blank">
                <svg width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                    stroke-linecap="round" stroke-linejoin="round" style="margin-right: 6px;">
                    <path
                        d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22">
                    </path>
                </svg>
                GitHub
            </a>
        </div>
    </nav>

    <div class="docs-layout">
        <button class="mobile-menu-toggle" id="mobile-menu-toggle" aria-label="Toggle Table of Contents">
            <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"
                stroke-linecap="round" stroke-linejoin="round">
                <line x1="3" y1="12" x2="21" y2="12"></line>
                <line x1="3" y1="6" x2="21" y2="6"></line>
                <line x1="3" y1="18" x2="21" y2="18"></line>
            </svg>
            Table of Contents
        </button>

        <aside class="sidebar" id="docs-sidebar">
            <div class="sidebar-section">
                <h4>Getting Started</h4>
                <a href="#installation" class="active">Installation</a>
                <a href="#quick-start">Quick Start</a>
                <a href="#packages">Which Package?</a>
            </div>
            <div class="sidebar-section">
                <h4>Core Concepts</h4>
                <a href="#pipeline">The Pipeline</a>
                <a href="#format-hierarchy">Format Hierarchy</a>
                <a href="#contex-compact">Contex Compact</a>
                <a href="#tens-text-format">TENS-Text</a>
                <a href="#output-formats">All Output Formats</a>
                <a href="#determinism">Determinism & Caching</a>
            </div>
            <div class="sidebar-section">
                <h4>SDK Middleware</h4>
                <a href="#openai">OpenAI</a>
                <a href="#anthropic">Anthropic</a>
                <a href="#gemini">Gemini</a>
                <a href="#placeholder-syntax">Placeholder Syntax</a>
            </div>
            <div class="sidebar-section">
                <h4>Adapters</h4>
                <a href="#langchain">LangChain</a>
                <a href="#llamaindex">LlamaIndex</a>
            </div>
            <div class="sidebar-section">
                <h4>API Reference</h4>
                <a href="#tens-api">Tens Class</a>
                <a href="#format-output">formatOutput()</a>
                <a href="#token-memory">TokenMemory</a>
                <a href="#engine">@contex-llm/engine</a>
            </div>
            <div class="sidebar-section">
                <h4>CLI</h4>
                <a href="#cli-commands">Commands</a>
            </div>
            <div class="sidebar-section">
                <h4>Strategy</h4>
                <a href="#when-to-use">When to Use Contex</a>
                <a href="#when-not">When NOT to Use</a>
                <a href="#migration">Migration from JSON</a>
            </div>
            <div class="sidebar-section">
                <h4>Results</h4>
                <a href="#benchmarks">Benchmarks</a>
                <a href="#architecture">Architecture</a>
            </div>
            <div class="sidebar-section">
                <h4>Advanced</h4>
                <a href="#edge-cases">Edge Cases</a>
                <a href="#performance">Performance Tips</a>
                <a href="#dictionary-tuning">Dictionary Tuning</a>
                <a href="#troubleshooting">Troubleshooting</a>
            </div>
        </aside>

        <main class="docs-content">
            <h1>Contex Documentation</h1>
            <p>Contex is an intelligent context engine for LLMs. It stores structured data as canonical IR (TENS) and
                serves it in the most token-efficient format — <strong>Contex Compact</strong> — saving
                <strong>46-90% of context window costs</strong>.</p>

            <!-- ═══════════════ GETTING STARTED ═══════════════ -->

            <h2 id="installation">Installation</h2>

            <h3>Package Manager</h3>
            <pre>pnpm add @contex-llm/core @contex-llm/middleware</pre>

            <h3>With Engine (cost analysis, auto-selection)</h3>
            <pre>pnpm add @contex-llm/core @contex-llm/engine @contex-llm/middleware</pre>

            <h3>CLI (Optional)</h3>
            <pre>pnpm add -g @contex-llm/cli</pre>

            <div class="callout info">
                <strong>Requirements:</strong> Node.js 18+ · TypeScript 5+
            </div>

            <hr class="divider">

            <h2 id="quick-start">Quick Start — 3 Lines</h2>

            <h3>1. Encode Your Data</h3>
            <pre>import { Tens } from '@contex-llm/core';

const tens = Tens.encode([
  { id: 1, name: 'Alice', role: 'admin', city: 'New York' },
  { id: 2, name: 'Bob', role: 'user', city: 'New York' },
  { id: 3, name: 'Charlie', role: 'admin', city: 'London' },
]);</pre>

            <h3>2. Get Optimized Output</h3>
            <pre>// Default: Contex Compact (recommended — best token savings)
const text = tens.toString();
// → n	r	c
//   @f	n=name	r=role	c=city
//   @d	New York	admin
//   Alice	@1	@0
//   Bob	user	@0
//   Charlie	@1	London

// Or materialize as token IDs for direct injection
const tokens = tens.materialize('gpt-4o');</pre>

            <h3>3. Use Middleware (Recommended)</h3>
            <pre>import { createContexOpenAI } from '@contex-llm/middleware';

const client = createContexOpenAI(new OpenAI(), {
  data: { users: myUserData }
});

await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{ role: 'user', content: 'Analyze {{CONTEX:users}}' }]
});</pre>

            <div class="callout tip">
                <strong>Result:</strong> 46-90% token reduction, deterministic output (100% cache hit rate), faster responses.
            </div>

            <hr class="divider">

            <h2 id="packages">Which Package Do I Need?</h2>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Package</th>
                        <th>When to Use</th>
                        <th>Key APIs</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><code>@contex-llm/core</code></td>
                        <td>Most users — encoding, formatting, tokenization</td>
                        <td><code>Tens</code>, <code>formatOutput</code>, <code>TokenMemory</code></td>
                    </tr>
                    <tr>
                        <td><code>@contex-llm/engine</code></td>
                        <td>Budget analysis, auto format selection, multi-model</td>
                        <td><code>quick()</code>, <code>selectBestFormat()</code></td>
                    </tr>
                    <tr>
                        <td><code>@contex-llm/middleware</code></td>
                        <td>Drop-in SDK integration (OpenAI, Anthropic, Gemini)</td>
                        <td><code>createContexOpenAI()</code>, <code>createContexAnthropic()</code></td>
                    </tr>
                    <tr>
                        <td><code>@contex-llm/cli</code></td>
                        <td>CLI tools, benchmarking, analysis</td>
                        <td><code>contex analyze</code>, <code>contex bench</code></td>
                    </tr>
                    <tr>
                        <td><code>@contex-llm/adapters</code></td>
                        <td>LangChain / LlamaIndex integration</td>
                        <td><code>ContexCallbackHandler</code>, <code>ContexNodePostprocessor</code></td>
                    </tr>
                </tbody>
            </table>

            <!-- ═══════════════ CORE CONCEPTS ═══════════════ -->

            <hr class="divider">

            <h2 id="pipeline">The Pipeline</h2>
            <p>Contex solves two problems: <strong>token waste</strong> (JSON syntax overhead consumes 30-60% of tokens)
                and <strong>non-deterministic tokenization</strong> (same data producing different tokens, breaking prefix cache).</p>

            <div class="format-pipeline">Your Data (JSON)
    │
    ▼
Tens.encode()  →  TENS Binary (Uint8Array)
                    │  Canonical IR — model-agnostic
                    │  SHA-256 content hash for dedup
                    │  Never sent to LLMs directly
                    │
    ┌───────────────┼───────────────┐
    ▼               ▼               ▼
Contex Compact   TENS-Text        Token Array
tens.toString()  formatOutput(    tens.materialize(
                  data,             'gpt-4o')
                  'tens-text')
│                │                 │
▼                ▼                 ▼
SEND TO LLM     DEBUG / INSPECT   DIRECT TOKEN
@d, @f, tabs    @dict, @schema    INJECTION
46-90% savings  Human-readable    Raw token IDs</div>

            <div class="callout warn">
                <strong>Key insight:</strong> TENS, TENS-Text, and Contex Compact are <em>different layers</em> of the same pipeline, not competing formats.
                TENS Binary is the internal IR. TENS-Text is for human debugging. <strong>Contex Compact is what you send to LLMs.</strong>
            </div>

            <hr class="divider">

            <h2 id="format-hierarchy">Format Hierarchy — Why Three "Formats"?</h2>

            <p>Think of it like a compiler:</p>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Traditional Compiler</th>
                        <th>Contex Pipeline</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Source code</td>
                        <td>Your JSON data</td>
                    </tr>
                    <tr>
                        <td>Object files (.o)</td>
                        <td>TENS Binary (Uint8Array, SHA-256 hash)</td>
                    </tr>
                    <tr>
                        <td>Assembly listing</td>
                        <td>TENS-Text (.tens) — human-readable IR</td>
                    </tr>
                    <tr>
                        <td>Optimized executable</td>
                        <td><strong>Contex Compact</strong> — optimized LLM output</td>
                    </tr>
                </tbody>
            </table>

            <div class="grid-2">
                <div class="format-card">
                    <h4>TENS Binary <span class="tag tag-internal">Internal</span></h4>
                    <p>Canonical Intermediate Representation. <code>Uint8Array</code> with SHA-256 content hash.</p>
                    <p>Used for: storage, deduplication, caching. You never see this directly.</p>
                    <pre>const tens = Tens.encode(data);
tens.ir;   // Uint8Array
tens.hash; // "abc123..."</pre>
                </div>
                <div class="format-card">
                    <h4>TENS-Text (.tens) <span class="tag tag-debug">Debug</span></h4>
                    <p>Human-readable serialization of TENS. Uses <code>@dict</code>, <code>@schema</code>, <code>@version</code>.</p>
                    <p>Used for: debugging, inspection, version control diffs.</p>
                    <pre>formatOutput(data, 'tens-text');
// @version 1
// @schema row name:str age:num
// row
//   name Alice
//   age 30</pre>
                </div>
            </div>

            <div class="format-card">
                <h4>Contex Compact <span class="tag tag-default">Default — Send to LLMs</span></h4>
                <p>The ultra-efficient output format. Uses <code>@d</code> (dictionary), <code>@f</code> (field map). Tab-separated, no brackets, no quotes.</p>
                <p><strong>This is what you send to LLMs.</strong> 46-90% token savings vs JSON.</p>
                <pre>tens.toString(); // or formatOutput(data, 'contex')
// n	r	c
// @f	n=name	r=role	c=city
// @d	New York	admin
// Alice	@1	@0
// Bob	user	@0
// Charlie	@1	London</pre>
            </div>

            <div class="callout info">
                <strong>Why different directive names?</strong> Contex Compact uses short directives (<code>@d</code>, <code>@f</code>) because every character costs tokens in LLM context.
                TENS-Text uses full names (<code>@dict</code>, <code>@schema</code>) because it's for human reading. Different formats, different audiences.
            </div>

            <hr class="divider">

            <h2 id="contex-compact">Contex Compact — Deep Dive</h2>

            <p>Contex Compact applies <strong>11 optimizations</strong> to squeeze maximum value from every token:</p>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>#</th>
                        <th>Optimization</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr><td>1</td><td>Deep flattening</td><td><code>user.address.city</code> → dot-notation keys</td></tr>
                    <tr><td>2</td><td>Field name compression</td><td><code>customer_shipping_address</code> → <code>c</code></td></tr>
                    <tr><td>3</td><td>Tab-separated schema</td><td>Schema declared once in header row</td></tr>
                    <tr><td>4</td><td>Dictionary compression</td><td>Repeated values → <code>@0</code>, <code>@1</code>, ...</td></tr>
                    <tr><td>5</td><td>Numeric dictionary</td><td>Repeated numbers (3+ chars) → <code>@N</code></td></tr>
                    <tr><td>6</td><td>Boolean abbreviation</td><td><code>true</code> → <code>T</code>, <code>false</code> → <code>F</code></td></tr>
                    <tr><td>7</td><td>Null abbreviation</td><td><code>null</code> / empty → <code>_</code></td></tr>
                    <tr><td>8</td><td>Integer shortening</td><td><code>42.0</code> → <code>42</code></td></tr>
                    <tr><td>9</td><td>Array compaction</td><td><code>[a, b, c]</code> → <code>a b c</code></td></tr>
                    <tr><td>10</td><td>Sparse mode</td><td>&gt;50% nulls → only non-null values with indices</td></tr>
                    <tr><td>11</td><td>No syntax overhead</td><td>No brackets, quotes, colons, or commas</td></tr>
                </tbody>
            </table>

            <h3>Before & After</h3>
            <pre>// JSON (139 tokens)
[{"id":1,"name":"Alice","role":"admin","city":"New York"},
 {"id":2,"name":"Bob","role":"user","city":"New York"},
 {"id":3,"name":"Charlie","role":"admin","city":"London"}]

// Contex Compact (58 tokens — 58% savings)
i	n	r	c
@f	i=id	n=name	r=role	c=city
@d	New York	admin
1	Alice	@1	@0
2	Bob	user	@0
3	Charlie	@1	London</pre>

            <hr class="divider">

            <h2 id="tens-text-format">TENS-Text (.tens)</h2>
            <p>The human-readable serialization of the TENS binary IR. Lossless 1:1 roundtrip with TENS Binary.</p>
            <p>Full documentation: <a href="./tens-text.html">TENS-Text Reference →</a></p>

            <pre>import { formatOutput } from '@contex-llm/core';

const tensText = formatOutput(data, 'tens-text');
// @version 1
// @encoding o200k_base
// @schema row name:str age:num city:str
// @dict New York
// row
//   name Alice
//   age 30
//   city @0</pre>

            <div class="callout info">
                <strong>When to use TENS-Text:</strong> Debugging your pipeline, inspecting what the IR contains, storing human-readable snapshots, version control diffs.
                <strong>Don't send this to LLMs</strong> — use Contex Compact instead.
            </div>

            <hr class="divider">

            <h2 id="output-formats">All Output Formats</h2>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Format</th>
                        <th>Code</th>
                        <th>Best For</th>
                        <th>Avg Savings</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Contex Compact</strong></td>
                        <td><code>'contex'</code></td>
                        <td>Everything — best overall for LLMs</td>
                        <td class="saving">72%</td>
                    </tr>
                    <tr>
                        <td>TOON</td>
                        <td><code>'toon'</code></td>
                        <td>Simple tabular (no dictionary)</td>
                        <td>~25%</td>
                    </tr>
                    <tr>
                        <td>CSV</td>
                        <td><code>'csv'</code></td>
                        <td>Flat tabular data</td>
                        <td>~38%</td>
                    </tr>
                    <tr>
                        <td>Markdown</td>
                        <td><code>'markdown'</code></td>
                        <td>Human-readable reports</td>
                        <td>~6%</td>
                    </tr>
                    <tr>
                        <td>JSON</td>
                        <td><code>'json'</code></td>
                        <td>Baseline / compatibility</td>
                        <td>0%</td>
                    </tr>
                    <tr>
                        <td>TENS-Text</td>
                        <td><code>'tens-text'</code></td>
                        <td>Debugging / inspection</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>Tokens</td>
                        <td><code>'tokens'</code></td>
                        <td>Direct token injection</td>
                        <td>N/A</td>
                    </tr>
                </tbody>
            </table>

            <pre>import { formatOutput } from '@contex-llm/core';

const compact = formatOutput(data, 'contex');   // Best overall
const csv     = formatOutput(data, 'csv');       // Flat data
const toon    = formatOutput(data, 'toon');      // Simple tabular
const md      = formatOutput(data, 'markdown');  // Reports
const json    = formatOutput(data, 'json');      // Baseline

// Or use Tens for default Contex Compact
const tens = Tens.encode(data);
const text = tens.toString(); // Always Contex Compact</pre>

            <hr class="divider">

            <h2 id="determinism">Determinism & Caching</h2>
            <p>Contex guarantees <strong>deterministic output</strong> — critical for LLM prefix caching (providers like OpenAI and Anthropic cache by token prefix).</p>

            <pre>// Different input key order → same output
const data1 = [{ b: 1, a: 2 }];
const data2 = [{ a: 2, b: 1 }];

Tens.encode(data1).toString() === Tens.encode(data2).toString();
// → true (same hash, same cache key)</pre>

            <p>This means: if you encode the same data twice, you get the same tokens. Prefix caching just works.</p>

            <!-- ═══════════════ SDK MIDDLEWARE ═══════════════ -->

            <hr class="divider">

            <h2 id="openai">OpenAI Middleware</h2>
            <pre>import OpenAI from 'openai';
import { createContexOpenAI } from '@contex-llm/middleware';

const client = createContexOpenAI(new OpenAI(), {
  data: { tickets: ticketData }
});

const response = await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [
    { role: 'system', content: 'You are a support agent.' },
    { role: 'user', content: 'Summarize {{CONTEX:tickets}}' }
  ]
});</pre>

            <h2 id="anthropic">Anthropic Middleware</h2>
            <p>Contex automatically adds <code>cache_control</code> breakpoints for Anthropic's prompt caching.</p>
            <pre>import Anthropic from '@anthropic-ai/sdk';
import { createContexAnthropic } from '@contex-llm/middleware';

const client = createContexAnthropic(new Anthropic(), {
  data: { context: kbData }
});

const message = await client.messages.create({
  model: 'claude-sonnet-4-20250514',
  max_tokens: 1024,
  messages: [{ role: 'user', content: 'Answer using {{CONTEX:context}}' }]
});</pre>

            <h2 id="gemini">Gemini Middleware</h2>
            <pre>import { createContexGemini } from '@contex-llm/middleware';

const client = createContexGemini(genai, {
  data: { documents: docsData }
});

const result = await client.models.generateContent({
  model: 'gemini-2.5-flash',
  contents: 'Summarize {{CONTEX:documents}}'
});</pre>

            <h2 id="placeholder-syntax">Placeholder Syntax</h2>
            <pre>{{CONTEX:key}}                    // Basic — replaced with Contex Compact output
{{CONTEX:key:maxRows=100}}       // Limit rows
{{CONTEX:key:format=csv}}        // Force specific format</pre>

            <div class="callout tip">
                <strong>How it works:</strong> The middleware intercepts your API call, finds <code>{{CONTEX:...}}</code> placeholders,
                replaces them with optimized Contex Compact output, then forwards to the provider.
            </div>

            <!-- ═══════════════ ADAPTERS ═══════════════ -->

            <hr class="divider">

            <h2 id="langchain">LangChain Adapter</h2>
            <pre>import { ContexCallbackHandler } from '@contex-llm/adapters';

const handler = new ContexCallbackHandler({
  data: { docs: myDocs },
  format: 'contex'  // Default
});

// Use with any LangChain chain
const chain = new LLMChain({ llm, prompt });
const result = await chain.call(
  { input: 'Analyze {{CONTEX:docs}}' },
  [handler]
);</pre>

            <h2 id="llamaindex">LlamaIndex Adapter</h2>
            <pre>import { ContexNodePostprocessor } from '@contex-llm/adapters';

const postprocessor = new ContexNodePostprocessor({
  format: 'contex'
});

// Use with LlamaIndex query engine
const engine = index.asQueryEngine({
  nodePostprocessors: [postprocessor]
});</pre>

            <!-- ═══════════════ API REFERENCE ═══════════════ -->

            <hr class="divider">

            <h2 id="tens-api">Tens Class</h2>
            <pre>import { Tens } from '@contex-llm/core';

// Encode data → TENS Binary
const tens = Tens.encode(data);

// Get optimized text (Contex Compact)
const text = tens.toString();

// Materialize as token IDs for a specific model
const result = tens.materialize('gpt-4o');
console.log(result.tokenCount);

// Content hash (for caching/dedup)
const hash = tens.hash; // SHA-256</pre>

            <h2 id="format-output">formatOutput()</h2>
            <pre>import { formatOutput } from '@contex-llm/core';

// Format to any output format
const contex   = formatOutput(data, 'contex');    // Contex Compact
const tensText = formatOutput(data, 'tens-text'); // TENS-Text
const csv      = formatOutput(data, 'csv');
const toon     = formatOutput(data, 'toon');
const md       = formatOutput(data, 'markdown');
const json     = formatOutput(data, 'json');

// Input validation: handles null, undefined, non-array gracefully
formatOutput(null, 'contex');    // → ''
formatOutput([], 'contex');      // → ''</pre>

            <h2 id="token-memory">TokenMemory</h2>
            <pre>import { TokenMemory } from '@contex-llm/core';

const memory = new TokenMemory('./.contex');

// Store and cache
const { hash } = memory.store(data);
const tokens = memory.materializeAndCache(hash, 'gpt-4o');

// Retrieve cached tokens (instant)
const cached = memory.getCached(hash, 'gpt-4o');</pre>

            <h2 id="engine">@contex-llm/engine</h2>
            <pre>import { quick, selectBestFormat } from '@contex-llm/engine';

// One-shot encode + materialize
const result = quick(data, 'gpt-4o');
console.log(result.tokens);
console.log(result.asText());

// Auto-select best format for data + model
const decision = selectBestFormat({ model: 'gpt-4o', data });
// → { format: 'contex', reason: '...', estimatedSavings: 0.45 }</pre>

            <!-- ═══════════════ CLI ═══════════════ -->

            <hr class="divider">

            <h2 id="cli-commands">CLI Commands</h2>
            <pre># Analyze savings across formats
contex analyze data.json --model gpt-4o
            contex analyze data.json --fingerprint

# Check estimated token savings
contex savings data.json

            # Validate roundtrip with structural checks + fingerprint
            contex validate data.json --semantic-guard --fingerprint
            contex validate data.json --fingerprint --no-watermark

# Materialize tokens (creates .contex/ cache)
contex materialize data.json --model gpt-4o

# Inject into prompt template
contex inject template.txt --data data.json

# Run benchmark suite
contex bench

# Cache diagnostics
contex cache-diagnose
contex cache-stats
contex cache-warm data.json --model gpt-4o</pre>

            <p style="color: var(--text-secondary); margin-top: 12px;">
                <strong>New observability output:</strong> <code>analyze</code> and <code>validate</code> now print pipeline CPU/heap/throughput reports,
                structural complexity scoring, and optional fingerprint/watermark metadata.
            </p>
            <p style="color: var(--text-secondary); margin-top: 8px;">
                If your terminal shows garbled box characters, add <code>--ascii</code> to any command.
            </p>

            <!-- ═══════════════ STRATEGY ═══════════════ -->

            <hr class="divider">

            <h2 id="when-to-use">When to Use Contex</h2>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Contex Savings</th>
                        <th>Verdict</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Repetitive structured data (DB records, API responses)</td>
                        <td class="saving">60-94%</td>
                        <td>Use Contex</td>
                    </tr>
                    <tr>
                        <td>Deeply nested objects</td>
                        <td class="saving">80-94%</td>
                        <td>Use Contex (flattening wins big)</td>
                    </tr>
                    <tr>
                        <td>Large datasets (100+ rows)</td>
                        <td class="saving">50-80%</td>
                        <td>Use Contex</td>
                    </tr>
                    <tr>
                        <td>Flat tabular with stable columns</td>
                        <td>~38%</td>
                        <td>Contex or CSV both work</td>
                    </tr>
                    <tr>
                        <td>Need deterministic cache hits</td>
                        <td>—</td>
                        <td>Use Contex (only option with determinism)</td>
                    </tr>
                </tbody>
            </table>

            <h2 id="when-not">When NOT to Use Contex</h2>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Scenario</th>
                        <th>Recommendation</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Free-text / unstructured data</td>
                        <td>Pass as-is (no structure to optimize)</td>
                    </tr>
                    <tr>
                        <td>&lt;5 rows of data</td>
                        <td>JSON is fine (overhead of headers not worth it)</td>
                    </tr>
                    <tr>
                        <td>&gt;80% sparse (mostly nulls)</td>
                        <td>JSON may be simpler (Contex sparse mode helps but limited)</td>
                    </tr>
                    <tr>
                        <td>LLM needs to parse structured output</td>
                        <td>Use JSON (LLMs parse JSON natively)</td>
                    </tr>
                    <tr>
                        <td>All values are unique strings</td>
                        <td>CSV (no dictionary compression opportunity)</td>
                    </tr>
                </tbody>
            </table>

            <h2 id="migration">Migration from JSON</h2>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Aspect</th>
                        <th>JSON</th>
                        <th>Contex</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Token usage</td>
                        <td>100% (baseline)</td>
                        <td class="saving">57-60%</td>
                    </tr>
                    <tr>
                        <td>Cache hits</td>
                        <td>None</td>
                        <td>100% deterministic</td>
                    </tr>
                    <tr>
                        <td>Cost per 1M tokens</td>
                        <td>$10</td>
                        <td>$4-6</td>
                    </tr>
                    <tr>
                        <td>Setup effort</td>
                        <td>None</td>
                        <td>3 lines</td>
                    </tr>
                </tbody>
            </table>

            <pre>// Before: Raw JSON in prompt
const response = await openai.chat.completions.create({
  model: 'gpt-4o',
  messages: [{
    role: 'user',
    content: `Data: ${JSON.stringify(myData)}\n\nAnalyze this.`
  }]
});

// After: Contex middleware (3-line change)
import { createContexOpenAI } from '@contex-llm/middleware';

const client = createContexOpenAI(openai, { data: { context: myData } });

const response = await client.chat.completions.create({
  model: 'gpt-4o',
  messages: [{
    role: 'user',
    content: 'Data: {{CONTEX:context}}\n\nAnalyze this.'
  }]
});</pre>

            <!-- ═══════════════ BENCHMARKS ═══════════════ -->

            <hr class="divider">

            <h2 id="benchmarks">Benchmark Results</h2>
            <p>Benchmark v8 — 21 dataset types, 36/36 tests passing, 20/20 data fidelity.</p>

            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>Value</th>
                        <th>Details</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Avg Pipeline Savings</td>
                        <td class="saving">72%</td>
                        <td>Across 21 dataset types</td>
                    </tr>
                    <tr>
                        <td>Best (DeepNested)</td>
                        <td class="saving">90%</td>
                        <td>Format-level savings</td>
                    </tr>
                    <tr>
                        <td>RealWorld Data</td>
                        <td class="saving">68%</td>
                        <td>Production-like ticket data</td>
                    </tr>
                    <tr>
                        <td>Data Fidelity</td>
                        <td class="saving">20/20</td>
                        <td>100% roundtrip accuracy</td>
                    </tr>
                    <tr>
                        <td>Edge-Case Tests</td>
                        <td class="saving">47/47</td>
                        <td>Nested, unicode, sparse, crash safety</td>
                    </tr>
                </tbody>
            </table>

            <p><a href="./benchmarks.html">Full interactive benchmark results →</a></p>

            <hr class="divider">

            <h2 id="architecture">Architecture</h2>
            <pre>@contex-llm/
├── core/           # Tens encoder, IR, formatters, tokenization
├── engine/         # Budget engine, quick() API, model registry
├── middleware/     # OpenAI, Anthropic, Gemini SDK wrappers
├── cli/            # CLI tools, benchmarks
├── adapters/       # LangChain & LlamaIndex integration
├── server/         # REST API server
└── tens-wasm/      # Rust/WASM encoder (future)</pre>

            <div class="callout info">
                <strong>More detailed docs (Markdown):</strong>
                <a href="https://github.com/kshitijpalsinghtomar/contex-llm/blob/main/docs/architecture.md">Architecture Deep Dive</a> ·
                <a href="https://github.com/kshitijpalsinghtomar/contex-llm/blob/main/docs/tens-specification.md">TENS Specification</a> ·
                <a href="https://github.com/kshitijpalsinghtomar/contex-llm/blob/main/docs/guide/examples.md">Examples</a> ·
                <a href="https://github.com/kshitijpalsinghtomar/contex-llm/blob/main/docs/guide/migration-from-json.md">Migration Guide</a> ·
                <a href="https://github.com/kshitijpalsinghtomar/contex-llm/blob/main/docs/reference/formats.md">Format Reference</a>
            </div>

            <!-- ═══════════════ ADVANCED ═══════════════ -->

            <hr class="divider">

            <h2 id="edge-cases">Edge Cases & Safety</h2>
            <p>Contex handles 47 edge-case scenarios out of the box. Here are the most important ones:</p>

            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Edge Case</th>
                        <th>How Contex Handles It</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Circular references in objects</td>
                        <td><code>flattenObject</code> uses cycle detection with a visited <code>Set</code> — circular paths are truncated safely</td>
                    </tr>
                    <tr>
                        <td>Unicode strings (emoji, CJK, RTL)</td>
                        <td>Fully supported — UTF-8 encoding preserved, no mojibake. Tab/newline characters in values are escaped (<code>\t</code>, <code>\n</code>)</td>
                    </tr>
                    <tr>
                        <td>Values that look like dictionary refs (<code>@0</code>, <code>@1</code>)</td>
                        <td>Automatically escaped with backslash prefix (<code>\@0</code>) to prevent ambiguity</td>
                    </tr>
                    <tr>
                        <td>Markdown pipe characters in values</td>
                        <td>Escaped in Markdown format output to prevent table corruption</td>
                    </tr>
                    <tr>
                        <td>Empty arrays / null datasets</td>
                        <td>Returns empty string gracefully — no crashes, no exceptions</td>
                    </tr>
                    <tr>
                        <td>Deeply nested objects (10+ levels)</td>
                        <td>Deep flattening with dot-notation handles arbitrary depth. Cycle detection prevents stack overflow</td>
                    </tr>
                    <tr>
                        <td>Mixed types in same column</td>
                        <td>Values are serialized to their string representation — type heterogeneity is safe</td>
                    </tr>
                    <tr>
                        <td>Very large datasets (10K+ rows)</td>
                        <td>Dictionary capped at MAX_DICT_SIZE=10,000 entries with cost-benefit gating to prevent bloated headers</td>
                    </tr>
                </tbody>
            </table>

            <hr class="divider">

            <h2 id="performance">Performance Tips</h2>

            <h3>1. Pre-compile with TokenMemory</h3>
            <p>For data that doesn't change between requests (product catalogs, knowledge bases), compile once and cache:</p>
            <pre>const memory = new TokenMemory('./.contex');
const { hash } = memory.store(productCatalog);

// Every subsequent request: ~5ms instead of ~200ms
const tokens = memory.materializeAndCache(hash, 'gpt-4o');</pre>

            <h3>2. Use the Engine for Auto-Selection</h3>
            <p>Don't guess the format — let the engine pick the optimal one for your data shape and model:</p>
            <pre>import { selectBestFormat } from '@contex-llm/engine';

const decision = selectBestFormat({ model: 'gpt-4o', data: myData });
// → { format: 'contex', reason: 'Nested data benefits from dict+flatten' }</pre>

            <h3>3. Batch Similar Data</h3>
            <p>Contex's dictionary compression works best when rows share values. Group similar records together for maximum savings — e.g., all users from the same department, all products in the same category.</p>

            <h3>4. Leverage Incremental Encoding</h3>
            <p>For multi-turn conversations, use <code>encodeIncremental()</code> to send only new rows:</p>
            <pre>import { StructuralDedupCache } from '@contex-llm/core';

const cache = new StructuralDedupCache();
const result1 = cache.encodeIncremental(batch1);
// Later: only new rows are encoded
const result2 = cache.encodeIncremental(batch2);
// result2.deltaRows = only the new rows</pre>

            <h3>5. WASM for CPU-Bound Encoding</h3>
            <p>If you're encoding large datasets on the hot path, the WASM encoder (when available) provides near-native speed in Node.js and browser environments.</p>

            <hr class="divider">

            <h2 id="dictionary-tuning">Dictionary Configuration</h2>
            <p>The dictionary is the biggest lever for token savings. Understanding how it works helps you structure data for maximum compression.</p>

            <h3>How Dictionary Compression Works</h3>
            <pre>// Input: 3 users, all from "New York", 2 are "admin"
// Dictionary detects repeated values and assigns short codes:
// @d  New York  admin
//
// Each occurrence of "New York" becomes @0 (saves 7 chars per use)
// Each occurrence of "admin" becomes @1 (saves 4 chars per use)</pre>

            <h3>When Dictionary Helps Most</h3>
            <table class="benchmark-table">
                <thead>
                    <tr>
                        <th>Data Pattern</th>
                        <th>Dictionary Savings</th>
                        <th>Example</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Enum-like columns (status, role, type)</td>
                        <td class="saving">High</td>
                        <td>50 rows × "active"/"inactive" → @0/@1</td>
                    </tr>
                    <tr>
                        <td>Shared references (city, country, department)</td>
                        <td class="saving">High</td>
                        <td>100 users in 5 cities → 5 dict entries</td>
                    </tr>
                    <tr>
                        <td>Repeated numeric values</td>
                        <td>Medium</td>
                        <td>Price tiers: 9.99, 19.99, 29.99</td>
                    </tr>
                    <tr>
                        <td>All unique strings</td>
                        <td>None</td>
                        <td>UUIDs, timestamps — no repetition</td>
                    </tr>
                </tbody>
            </table>

            <h3>Scalability Safeguards</h3>
            <p>To prevent dictionary headers from consuming more tokens than they save:</p>
            <ul>
                <li><strong>MAX_DICT_SIZE = 10,000</strong> — Hard cap on dictionary entries</li>
                <li><strong>Cost-benefit gate</strong> — A value only enters the dictionary if <code>(occurrences × value.length) > (referenceLength + headerCost)</code></li>
                <li><strong>Minimum frequency</strong> — Values appearing only once are never added to the dictionary</li>
            </ul>

            <hr class="divider">

            <h2 id="troubleshooting">Troubleshooting</h2>

            <h3>Common Issues</h3>

            <details class="faq-item" style="margin-bottom: 16px;">
                <summary class="faq-question">The LLM doesn't understand the Contex Compact format</summary>
                <div class="faq-answer">
                    This is rare with modern models (GPT-4o, Claude 3.5+, Gemini 2+). If it happens:
                    <ol>
                        <li>Add a brief instruction: <em>"The data below uses Contex Compact format. @d is the dictionary, @f maps short field names to full names. Each row is tab-separated."</em></li>
                        <li>Try <code>formatOutput(data, 'csv')</code> as a fallback — slightly less savings but universally understood</li>
                        <li>For older/smaller models, use <code>formatOutput(data, 'markdown')</code></li>
                    </ol>
                </div>
            </details>

            <details class="faq-item" style="margin-bottom: 16px;">
                <summary class="faq-question">Token count doesn't match what I expect</summary>
                <div class="faq-answer">
                    Contex uses real tokenizer models (o200k_base for GPT-4o, cl100k for GPT-3.5). Token counts are exact for supported models. For unsupported models, counts are estimated. Use the <a href="./playground.html">Playground</a> to verify, or paste the output into <a href="https://platform.openai.com/tokenizer" target="_blank">OpenAI's Tokenizer</a>.
                </div>
            </details>

            <details class="faq-item" style="margin-bottom: 16px;">
                <summary class="faq-question">My data has very low savings (&lt;20%)</summary>
                <div class="faq-answer">
                    Low savings usually means:
                    <ul>
                        <li><strong>Few rows</strong> — Header overhead is fixed. Below ~5 rows, the header eats into savings</li>
                        <li><strong>All unique values</strong> — No dictionary opportunity. Try CSV format instead</li>
                        <li><strong>Mostly string content</strong> — Long unique strings can't be compressed further. Contex optimizes <em>structure</em>, not content</li>
                    </ul>
                    Run <code>contex analyze data.json</code> to see a per-format breakdown and savings recommendation.
                </div>
            </details>

            <details class="faq-item" style="margin-bottom: 16px;">
                <summary class="faq-question">CLI shows garbled box-drawing characters</summary>
                <div class="faq-answer">
                    Your terminal may not support Unicode box-drawing characters. Add <code>--ascii</code> to any CLI command to use plain ASCII borders instead.
                </div>
            </details>

            <details class="faq-item" style="margin-bottom: 16px;">
                <summary class="faq-question">Middleware placeholder isn't being replaced</summary>
                <div class="faq-answer">
                    Check that:
                    <ol>
                        <li>The placeholder matches exactly: <code>{{CONTEX:keyName}}</code> (case-sensitive)</li>
                        <li>The key is registered in the <code>data</code> option: <code>data: { keyName: myData }</code></li>
                        <li>You're using the wrapped client (e.g., <code>createContexOpenAI()</code>), not the original</li>
                    </ol>
                </div>
            </details>

        </main>
    </div>

    <footer class="footer">
        <p>MIT © 2026 <a href="https://github.com/kshitijpalsinghtomar/contex-llm">Contex</a> · Created by <strong>Kshitij Pal Singh Tomar</strong> · Built for production LLM pipelines</p>
    </footer>

    <script>
        const toggleBtn = document.getElementById('theme-toggle');
        const sunIcon = toggleBtn.querySelector('.sun-icon');
        const moonIcon = toggleBtn.querySelector('.moon-icon');

        function updateIcon() {
            const isDark = document.documentElement.getAttribute('data-theme') === 'dark';
            sunIcon.style.display = isDark ? 'none' : 'block';
            moonIcon.style.display = isDark ? 'block' : 'none';
            toggleBtn.style.color = isDark ? 'var(--text-muted)' : 'var(--accent-amber)';
        }
        updateIcon();

        toggleBtn.addEventListener('click', () => {
            const current = document.documentElement.getAttribute('data-theme');
            const next = current === 'dark' ? 'light' : 'dark';
            document.documentElement.setAttribute('data-theme', next);
            localStorage.setItem('contex-theme', next);
            updateIcon();
        });

        // Mobile Menu Toggle
        const mobileMenuBtn = document.getElementById('mobile-menu-toggle');
        const sidebar = document.getElementById('docs-sidebar');

        if (mobileMenuBtn && sidebar) {
            mobileMenuBtn.addEventListener('click', () => {
                sidebar.classList.toggle('open');
                mobileMenuBtn.classList.toggle('active');
            });

            sidebar.querySelectorAll('a').forEach(link => {
                link.addEventListener('click', () => {
                    if (window.innerWidth <= 900) {
                        sidebar.classList.remove('open');
                        mobileMenuBtn.classList.remove('active');
                    }
                });
            });
        }

        // Scroll Spy
        const sections = document.querySelectorAll('h2[id]');
        const navLinks = document.querySelectorAll('.sidebar-section a');

        const sectionVisibility = new Map();

        const observer = new IntersectionObserver((entries) => {
            entries.forEach(entry => {
                sectionVisibility.set(entry.target.id, entry.isIntersecting);
            });

            let activeId = null;
            for (const section of sections) {
                if (sectionVisibility.get(section.id)) {
                    activeId = section.id;
                    break;
                }
            }

            if (!activeId) {
                let minDistance = Infinity;
                for (const section of sections) {
                    const rect = section.getBoundingClientRect();
                    const distance = Math.abs(rect.top);
                    if (distance < minDistance) {
                        minDistance = distance;
                        activeId = section.id;
                    }
                }
            }

            if (activeId) {
                navLinks.forEach(link => {
                    link.classList.remove('active');
                    if (link.getAttribute('href') === `#${activeId}`) {
                        link.classList.add('active');
                    }
                });
            }
        }, {
            rootMargin: '0px 0px -80% 0px',
            threshold: 0
        });

        sections.forEach(section => {
            observer.observe(section);
        });
    </script>
</body>

</html>

