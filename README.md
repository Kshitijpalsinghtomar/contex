<div align="center">

# Contex

# The Token-Native Data Infrastructure for AI Systems

**Reduce token volume by 46-90% before the tokenizer ever runs.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](./LICENSE)
[![TypeScript](https://img.shields.io/badge/%3C%2F%3E-TypeScript-blue.svg)](http://www.typescriptlang.org/)
[![npm version](https://img.shields.io/npm/v/@contex-llm/core.svg)](https://www.npmjs.com/package/@contex-llm/core)
[![Test Status](https://img.shields.io/badge/Tests-600%2B%20Passed-10b981.svg)]()

---

> **"You cannot modify the OpenAI tokenizer. But you CAN modify your structure."**

[![Get Started](https://img.shields.io/badge/ğŸš€-Quick_Start-blue.svg)](./docs/guide/getting-started.md)
[![View Benchmarks](https://img.shields.io/badge/ğŸ“Š-Benchmarks-blue.svg)](./docs/benchmarks.md)
[![Read the Docs](https://img.shields.io/badge/ğŸ“â€“-Documentation-blue.svg)](./docs/index.md)

</div>

---

## âš¡ Measured Snapshot (Benchmark v8)

Benchmark v8 covers 21 dataset types across multiple sizes, with 36/36 tests passing and 20/20 data fidelity checks.

| Metric | Value | Details |
| :--- | :--- | :--- |
| **Avg Pipeline Savings** | **72%** | Across 21 dataset types |
| **Best Format Savings** | **90%** (DeepNested) | Contex Compact format |
| **RealWorld Savings** | **68%** | Production-like ticket data |
| **Data Fidelity** | **20/20** | Perfect roundtrip accuracy |
| **WASM Acceleration** | **Active** | Rust-compiled encoder via WebAssembly |
| **Test Suite** | **600+ tests** | Across 7 packages |

> [!IMPORTANT]
> **Benchmark v8 evidence scope.**
> *   âœ… **Contex Compact format**: 7+ compression directives (@t, @d, @c, @p, @f, @sparse, bool/null)
> *   âœ… **WASM Acceleration**: Rust-compiled encoder for 2-5Ã— faster encoding
> *   âœ… **Type Safe**: Full TypeScript support with strict mode
> *   âœ… **Deterministic**: Stable canonical prefixes for prefix cache reuse
> *   âœ… **Multi-provider**: Works across OpenAI, Anthropic, and Gemini (39 models in registry)
> *   âœ… **compose() & quick()**: High-level APIs for prompt assembly and one-shot encoding
> *   âœ… **pipeline() helper**: Chained encode â†’ optimize â†’ materialize in one call
> *   âœ… **Verified**: 36/36 benchmark, 20/20 fidelity, 16/16 connectivity, 532 core tests


---

## ğŸš€ The Problem: Structural Bloat

Every LLM API call today suffers from structural inefficiency:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOUR PIPELINE TODAY                                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                          â”‚  
â”‚                                                                             â”‚
â”‚  Your App    â†’    JSON (Bloated)    â†’    Tokenizer    â†’    Inference        â”‚
â”‚                          â†'                                                  â”‚
â”‚                     30-60% of tokens                                        â”‚
â”‚                     are just SYNTAX                                         â”‚
â”‚                     (brackets, quotes,                                      â”‚
â”‚                      repeated keys)                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**What's happening:**
1.  **You pay for syntax**: Brackets, quotes, and whitespace consume tokens but add no information
2.  **You waste compute**: The provider re-tokenizes the same static keys billions of times
3.  **You break caching**: Non-deterministic JSON serialization kills your cache hit rate

---

## ğŸ’¡ The Solution: Prompt Structure Optimization

Contex inserts itself at the only layer you control: **Before the Tokenizer.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CONTAX OPTIMIZED PIPELINE                                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                               â”‚
â”‚                                                                                           â”‚
â”‚  Your App    â†’    Contex Compiler    â†’    Optimized    â†’    Tokenizer    â†’    Inference   â”‚
â”‚                                           Structure                                       â”‚
â”‚                       â†'                                                                   â”‚
â”‚                  46-90% reduction                                           â”‚
â”‚                  before we ever                                              â”‚
â”‚                  reach the tokenizer                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Contex compiles your data into **Canonical IR (TENS)** â€” a deterministic, model-agnostic format optimized to reduce structural token overhead.

### Why It Works

| Feature | Benefit |
|---------|---------|
| **Not Overfit** | Works across OpenAI, Anthropic, and Gemini |
| **Not Accidental** | Structural efficiency is fundamental, not a trick |
| **Infrastructure Grade** | 0ms latency penalty. Type-safe. Deterministic |

---

## ğŸ› ï¸ Quick Start

### Canonical Newcomer Flow (Default)

Use this single path first, then branch into advanced options only if needed.

1. `contex analyze` â€” verify reduction and strategy on your dataset
2. `contex materialize` â€” build model-specific cache artifacts
3. SDK inject with middleware â€” run prompts using `{{CONTEX:key}}`

### 1. Install

```bash
pnpm add @contex-llm/core @contex-llm/engine @contex-llm/middleware
```

### 2. Analyze Your Data (CLI)

```bash
npx contex analyze my_data.json --model gpt-4o
npx contex analyze my_data.json --model gpt-4o --fingerprint
```

For strict confidence gating:

```bash
npx contex analyze my_data.json --model gpt-4o-mini --strategy auto --auto-confidence-floor 55 --strict-auto-gate
npx contex analyze my_data.json --model gpt-4o-mini --fingerprint --no-watermark
```

**Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                     CONTEXT ANALYSIS REPORT                         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Input:          my_data.json                                        â•‘
â•‘  JSON Tokens:    39,605                                              â•‘
â•‘  Contex Tokens:  18,218  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  -72% ğŸŸ¢              â•‘
â•‘  Savings:        $5.47 per 1M requests                               â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Format Ranking:                                                     â•‘
â•‘    Contex Compact  72% saved (best overall)                          â•‘
â•‘    TOON            35% saved                                         â•‘
â•‘    CSV             38% saved                                         â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### 3. Integrate (SDK)

**One line of code** to enable structural optimization:

```typescript
import OpenAI from 'openai';
import { createContexOpenAI } from '@contex-llm/middleware';

// Wrap your client - that's it!
const client = createContexOpenAI(new OpenAI(), {
  data: { 
    users: myLargeDataset  // Automatically optimized
  }
});

// Use as normal - placeholders get replaced automatically
await client.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [{ 
    role: 'user', 
    content: 'Analyze these users: {{CONTEX:users}}' 
  }],
});
```

### Advanced Paths (Optional)

- Multi-dataset analysis: `contex savings data.json`
- Roundtrip validation: `contex validate data.json --semantic-guard`
- Fingerprint + watermark proof: `contex validate data.json --fingerprint`
- Fingerprint only (no watermark): `contex validate data.json --fingerprint --no-watermark`
- Full benchmark suite: `npx tsx packages/cli/src/benchmark.ts`

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CONTEX ARCHITECTURE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚   Your App   â”‚â”€â”€â”€â”€â–¶â”‚   Contex    â”‚â”€â”€â”€â”€â–¶â”‚    LLM       â”‚             â”‚
â”‚  â”‚  (Data In)   â”‚     â”‚   Compiler   â”‚     â”‚  (Optimized) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                               â”‚                                         â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚         â”‚                     â”‚                     â”‚                   â”‚
â”‚         â–¼                     â–¼                     â–¼                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   Canonical â”‚      â”‚  Materializeâ”‚      â”‚  Middleware â”‚              â”‚
â”‚  â”‚      IR     â”‚â”€â”€â”€â”€â”€â–¶â”‚   (Tokens)  â”‚â”€â”€â”€â”€â”€â–¶â”‚  (Injection)â”‚             â”‚
â”‚  â”‚  (TENS)     â”‚      â”‚             â”‚      â”‚             â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                     â”‚                     â”‚                   â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                               â”‚                                         â”‚
â”‚                               â–¼                                         â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                   â”‚
â”‚                      â”‚  .contex/    â”‚                                   â”‚
â”‚                      â”‚  (Cache)     â”‚                                   â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### The Five Layers

| Layer | Package | Description |
|-------|---------|-------------|
| **Layer 1: Canonical IR** | `@contex-llm/core` | Deterministic binary encoding (TENS) |
| **Layer 1b: WASM Acceleration** | `@contex-llm/tens-wasm` | Rust-compiled encoder for 2-5Ã— speedup |
| **Layer 2: Materialization** | `@contex-llm/core` | Model-specific token generation with 7+ Compact directives |
| **Layer 3: Composition** | `@contex-llm/engine` | compose(), quick(), pipeline(), budget validation |
| **Layer 4: Injection** | `@contex-llm/middleware` | Drop-in SDK wrappers for OpenAI, Anthropic, Gemini |

---

## ğŸ“Â¦ Packages

| Package | Description | Status |
|---------|-------------|--------|
| `@contex-llm/core` | Canonical IR, materializer, TokenMemory, WASM bridge | âœ… Stable |
| `@contex-llm/engine` | Budget engine, compose(), quick(), pipeline(), MODEL_REGISTRY (39 models) | âœ… Stable |
| `@contex-llm/middleware` | OpenAI, Anthropic, Gemini wrappers | âœ… Stable |
| `@contex-llm/cli` | CLI tools, benchmarks (21 datasets), ANSI colored output | âœ… Stable |
| `@contex-llm/tens-wasm` | Rust-compiled WASM encoder | âœ… Stable |
| `@contex-llm/adapters` | LangChain & LlamaIndex integrations | â¸ Paused |

---

## âœ… Production Ready (P0 Complete)

All critical features implemented and tested:

| Feature | Status | Description |
|---------|--------|-------------|
| **Streaming Support** | âœ… Complete | Works with OpenAI, Anthropic, Gemini streaming |
| **Error Handling & Validation** | âœ… Complete | 8+ custom error types |
| **Observability** | âœ… Complete | Built-in logging with CONTEX_DEBUG |
| **Test Coverage** | âœ… Complete | 600+ tests passing across 7 packages |

---

## ğŸ“Å¡ Documentation

| Guide | Description |
|-------|-------------|
| [Getting Started](./docs/guide/getting-started.md) | 5-minute quick start tutorial |
| [ğŸš€ Quickstart](./docs/guide/quickstart.md) | â­ New: 3-line workflow in under 10 minutes |
| [ğŸ“" Migration Guide](./docs/guide/migration-from-json.md) | Coming from JSON? Start here |
| [Architecture](./docs/architecture.md) | Deep dive into system design |
| [API Reference](./docs/reference/core.md) | Complete API documentation |
| [Benchmarks](./docs/benchmarks.md) | Performance benchmarks and methodology |
| [Examples](./docs/guide/examples.md) | Real-world use cases (including LangChain & LlamaIndex) |
| [Comparison](./docs/guide/comparison.md) | Contex vs JSON, MessagePack, Protobuf |

---

## ğŸ§ª Testing

```bash
# Run all tests
pnpm test

# Run benchmarks
pnpm bench

# Run linter
pnpm lint
```

---

## ğŸ¤ Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for guidelines.

---

## ğŸ“" License

MIT Â© Contex Team

---

<div align="center">

**Built with â¤ï¸ for the AI Developer Community**

[![GitHub Stars](https://img.shields.io/github/stars/kshitijpalsinghtomar/contex-llm?style=social)](https://github.com/kshitijpalsinghtomar/contex-llm)
[![Follow on X](https://img.shields.io/twitter/follow/contex?style=social)](https://twitter.com/contex)

</div>
