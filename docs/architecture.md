
<div align="center">

# Contex Architecture

> **The technical architecture of Contex â€” the token-native data infrastructure for AI systems.**

</div>

---


## ğŸ¯ Overview

Contex is a **token compiler** that transforms structured data into optimized, deterministic representations. Think of it like a traditional compiler:

| Traditional Compiler | Contex (Token Compiler) |
|---|---|
| Source code | Structured data (JSON, objects) |
| Compiler frontend | Canonical IR encoder |
| Object files (`.o`) | IR files (`.tens.ir`) |
| Linker | Token composer |
| Platform-specific binary | Model-specific token array |
| Runtime | LLM inference |


---

## Table of Contents

1. [The Problem](#the-problem)
2. [The Solution](#the-solution)
3. [Four Layers](#the-four-layers)
4. [Package Structure](#package-structure)
5. [Storage Layout](#storage-layout)
6. [Design Decisions](#design-decisions)

---

## The Problem

Every LLM API call today:

```
App sends JSON text â†’ Provider tokenizes it â†’ Token IDs â†’ Model processes
```

This is like shipping source code and recompiling on every request:
- âŒ **Wasted compute** â€” Identical data tokenized thousands of times a day
- âŒ **Cache misses** â€” Non-deterministic formatting breaks provider-side prefix caches
- âŒ **Token waste** â€” JSON syntax (brackets, quotes, repeated keys) consumes 30â€“60%

---

## The Solution

```
App encodes data ONCE as canonical IR
  â†’ Materialize to tokens (cached)
  â†’ Compose prompt from token blocks
  â†’ Inject canonical text (deterministic)
  â†’ Provider tokenizes identically
  â†’ Cache HIT guaranteed
```

**Encode once. Materialize per model. Compose instantly. Inject deterministically. Guarantee cache hits.**

---

## The Four Layers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 4: INJECTION                                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚
â”‚  createContexOpenAI() / createContexAnthropic() / createContexGemini() â”‚
â”‚  â†’ Drop-in SDK wrappers that auto-inject canonical text        â”‚
â”‚  â†’ Deterministic serialization guarantees provider cache hits  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†'
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 3: COMPOSITION                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚
â”‚  compose({ model, blocks, reserveForResponse })                â”‚
â”‚  â†’ Assemble prompts from pre-materialized token blocks         â”‚
â”‚  â†’ Validates total fits in model's context window              â”‚
â”‚  â†’ Deterministic token topology â†’ guaranteed cache hits        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†'
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 2: MEMORY + MATERIALIZATION                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚
â”‚  TokenMemory.store(data) â†’ customer_data.tens.ir (canonical)   â”‚
â”‚  TokenMemory.materializeAndCache(hash, model) â†’ token array    â”‚
â”‚  â†’ Content-addressed: same data = same hash = skip encoding    â”‚
â”‚  â†’ Model-specific blobs cached only for hot paths              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†'
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LAYER 1: CANONICAL IR ENCODING                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚
â”‚  encodeIR(data) â†’ { ir: Uint8Array, schema, hash }             â”‚
â”‚  â†’ Deterministic: sorted keys, canonical numbers, NFKC unicode â”‚
â”‚  â†’ Model-agnostic: no tokenizer dependency                     â”‚
â”‚  â†’ Schema-aware: field names, types, structure                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†'
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INPUT: Structured Data (JSON, objects, database rows)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Package Structure

```
contex/
â”œâ”€â”€ packages/
â”‚   â”œâ”€â”€ core/          â† Layer 1: TENS IR encoding/decoding, tokenization, schemas, canonicalization
â”‚   â”œâ”€â”€ engine/        â† Layers 2-3: Materialization, memory, composition, budget, model registry
â”‚   â”œâ”€â”€ middleware/     â† Layer 4: Drop-in SDK wrappers (OpenAI, Anthropic, Gemini)
â”‚   â”œâ”€â”€ cli/           â† Command-line tools
â”‚   â”œâ”€â”€ adapters/      â† LangChain & LlamaIndex integrations
â”‚   â”œâ”€â”€ server/        â† [PAUSED] REST API
â”‚   â””â”€â”€ tens-wasm/     â† [PAUSED] Rust/WASM
â”œâ”€â”€ docs/              â† Technical documentation
â”œâ”€â”€ website/           â† Marketing website
â””â”€â”€ CONTEX_V3_MASTER.md â† Single source of truth
```

### Package Responsibilities

| Package | Layer | Responsibility |
|---------|-------|----------------|
| `@contex-llm/core` | Layer 1 | Canonical IR encoder, materializer, tokenizer manager |
| `@contex-llm/engine` | Layers 2-3 | Budget engine, `quick()` API, model registry |
| `@contex-llm/middleware` | Layer 4 | OpenAI, Anthropic, Gemini SDK wrappers |
| `@contex-llm/cli` | Tools | CLI tools, benchmarks, cost analysis |

---

## Storage Layout

```
.contex/
â”œâ”€â”€ ir/
â”‚   â””â”€â”€ {sha256-hash}/
â”‚       â”œâ”€â”€ ir.bin       â† Canonical IR bytes (model-agnostic)
â”‚       â””â”€â”€ meta.json    â† Schema, row count, version
â””â”€â”€ cache/
    â””â”€â”€ {ir-hash}/
        â””â”€â”€ {model}.{encoding}.{version}/
            â”œâ”€â”€ tokens.bin  â† Int32Array binary cache
            â””â”€â”€ meta.json   â† Fingerprint, token count
```

### Why Binary Token Cache?

`tokens.bin` stores raw `Int32Array` buffers:
- **4x smaller** than JSON arrays
- **Instant to load** â€” buffer read, no parsing
- **Fingerprint validated** â€” auto-invalidates if tokenizer changes

---

## Design Decisions

### Why Not Store Per-Model Tokens Only?

If you encode data with OpenAI's `o200k_base` tokenizer and store those token IDs:
- âŒ Those tokens are **meaningless** to Claude (different tokenizer)
- âŒ If OpenAI updates their tokenizer, stored tokens become **invalid**
- âŒ Storing tokens for every model = storage explosion (10 models Ã— 10K files = 100K blobs)

### The Solution: Canonical IR + Lazy Materialization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CANONICAL IR (.tens.ir)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  Model-AGNOSTIC binary representation of your data               â”‚
â”‚  â€¢ Deterministic (same data = same bytes = same hash, ALWAYS)    â”‚
â”‚  â€¢ Contains: schema + values in canonical format                 â”‚
â”‚  â€¢ Does NOT contain token IDs â€” those are generated on demand    â”‚
â”‚  â€¢ Content-addressed by hash (skip re-encoding if hash exists)   â”‚
â”‚  â€¢ This is what you STORE and PERSIST                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MATERIALIZATION (per-model)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚
â”‚  TokenMemory.materializeAndCache(irHash, 'gpt-4o') â†’ number[]    â”‚
â”‚  â€¢ Takes canonical IR, tokenizes with target model's tokenizer   â”‚
â”‚  â€¢ Result is cached as a "hot blob" (.tens.gpt4o, .tens.claude)  â”‚
â”‚  â€¢ Cache invalidated when model tokenizer version changes        â”‚
â”‚  â€¢ Cold materialization: ~200ms. Warm (cached): <20ms.           â”‚
â”‚  â€¢ Only materialize for models you actually use (lazy)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Why This Matters Now (Market Proof)

| Provider | What they support | How TENS exploits it |
|----------|-------------------|----------------------|
| **OpenAI** | **Automatic prefix caching**: 50% discount on inputs >1024 tokens | Canonical text â†’ identical prefix tokens â†’ 100% cache hit rate |
| **Anthropic** | **Explicit prompt caching**: 90% cheaper for exact prefix match | Deterministic canonicalization = guaranteed exact prefix match |
| **Google** | **Implicit & Explicit caching**: 75-90% discount | Same: deterministic output = maximum implicit cache rates |

> **Note:** As of Feb 2026, no major provider accepts raw token arrays in their Chat APIs. Contex achieves the same result by injecting **canonical deterministic text** which produces identical tokens on the provider side.

---

## Related Documentation

- [TENS Specification](./tens-specification.md) â€” Binary format details
- [Getting Started](./guide/getting-started.md) â€” Quick tutorial
- [API Reference](./reference/core.md) â€” Complete API docs
- [CONTEX_V3_MASTER.md](../CONTEX_V3_MASTER.md) â€” Single source of truth
